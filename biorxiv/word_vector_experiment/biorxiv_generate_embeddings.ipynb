{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BioRxiv Document Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to generate document embeddings for every article in bioRxiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T14:09:57.230566Z",
     "start_time": "2020-05-13T14:09:55.636591Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"../../modules/\")\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm_notebook\n",
    "import umap\n",
    "\n",
    "from document_helper import generate_doc_vector, DocIterator, dump_article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T14:09:57.365818Z",
     "start_time": "2020-05-13T14:09:57.252428Z"
    }
   },
   "outputs": [],
   "source": [
    "journal_map_df = pd.read_csv(\"../exploratory_data_analysis/output/biorxiv_article_metadata.tsv\", sep=\"\\t\")\n",
    "journal_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T14:09:57.639128Z",
     "start_time": "2020-05-13T14:09:57.633972Z"
    }
   },
   "outputs": [],
   "source": [
    "biorxiv_xpath_str = \"//abstract/p|//abstract/title|//body/sec//p|//body/sec//title\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Documents to File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section dumps all of biorxiv text into a single document in order to train the word2vec model. This is for ease of training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T14:20:10.916307Z",
     "start_time": "2020-05-13T14:11:55.296993Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Only use the most current version of the documents\n",
    "latest_journal_version = (\n",
    "    journal_map_df.groupby(\"doi\")\n",
    "    .agg({\"document\":\"first\", \"doi\":\"last\"})\n",
    ")\n",
    "\n",
    "with open(\"output/word2vec_input/biorxiv_text.txt\", \"w\") as f:\n",
    "    for article in tqdm_notebook(latest_journal_version.document.tolist()):\n",
    "        document_text = dump_article_text(\n",
    "            file_path=f\"../biorxiv_articles/{article}\",\n",
    "            xpath_str=biorxiv_xpath_str,\n",
    "            remove_stop_words=True\n",
    "        )\n",
    "        \n",
    "        f.write(\"\\n\".join(document_text))\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section trains the word2vec model (continuous bag of words [CBOW]). Since the number of dimensions can vary I decided to train multiple models: 150, 250, 300. Each model is saved into is own respective directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T15:36:38.769999Z",
     "start_time": "2020-05-13T14:24:34.660854Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embedding_sizes = [150, 250, 300]\n",
    "for size in word_embedding_sizes:\n",
    "    print(size)\n",
    "    \n",
    "    #Create save path\n",
    "    word_path = Path(f\"output/word2vec_models/{size}\")\n",
    "    word_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Run Word2Vec\n",
    "    words = Word2Vec(DocIterator(\"output/word2vec_input/biorxiv_text.txt\"), size=size, iter=20, seed=100)\n",
    "    \n",
    "    #Save the model for future use\n",
    "    words.save(f\"{str(word_path.resolve())}/biorxiv_{size}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the word2vec models, the next step is to generate a document embeddings. For this experiment each document embedding is generated via an average of all word vectors contained in the document. There are better approaches towards doing this, but this can serve as a simple baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T15:44:14.736251Z",
     "start_time": "2020-05-13T15:41:46.334155Z"
    }
   },
   "outputs": [],
   "source": [
    "for word_model_path in Path().rglob(\"output/word2vec_models/*/*.model\"):\n",
    "    model_dim = word_model_path.parents[0].stem\n",
    "    word_model = Word2Vec.load(str(word_model_path.resolve()))\n",
    "    \n",
    "    biorxiv_document_map = {\n",
    "        document:generate_doc_vector(\n",
    "            word_model, \n",
    "            document_path = f\"../biorxiv_articles/{document}\",\n",
    "            xpath=biorxiv_xpath_str\n",
    "        )\n",
    "        for document in tqdm_notebook(journal_map_df.document.tolist())\n",
    "    }\n",
    "\n",
    "    biorxiv_vec_df = (\n",
    "        pd.DataFrame\n",
    "        .from_dict(biorxiv_document_map, orient=\"index\")\n",
    "        .rename(columns={col:f\"feat_{col}\" for col in range(int(model_dim))})\n",
    "        .rename_axis(\"document\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    biorxiv_vec_df.to_csv(\n",
    "        f\"output/word2vec_output/biorxiv_all_articles_{model_dim}.tsv.xz\", \n",
    "       sep=\"\\t\", index=False,\n",
    "        compression=\"xz\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating document embeddings, the next step is to visualize all the documents. In order to visualize the embeddings a low dimensional representation is needed. UMAP is an algorithm that can generate this representation, while grouping similar embeddings together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.228763Z",
     "start_time": "2020-03-20T14:53:26.467Z"
    }
   },
   "outputs": [],
   "source": [
    "random_state = 100\n",
    "n_neighbors = journal_map_df.category.unique().shape[0]\n",
    "n_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.229518Z",
     "start_time": "2020-03-20T14:53:26.469Z"
    }
   },
   "outputs": [],
   "source": [
    "for biorxiv_doc_vectors in Path().rglob(\"output/word2vec_output/biorxiv_all_articles*.tsv.xz\"):\n",
    "    model_dim = int(re.search(r\"(\\d+)\", biorxiv_doc_vectors.stem).group(1))\n",
    "    biorxiv_articles_df = pd.read_csv(str(biorxiv_doc_vectors.resolve()), sep=\"\\t\")\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_components=n_components, \n",
    "        n_neighbors=n_neighbors, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(\n",
    "        biorxiv_articles_df[[f\"feat_{idx}\" for idx in range(model_dim)]].values\n",
    "    )\n",
    "    \n",
    "    umapped_df = (\n",
    "        pd.DataFrame(embedding, columns=[\"umap1\", \"umap2\"])\n",
    "        .assign(document=biorxiv_articles_df.document.values.tolist())\n",
    "        .merge(journal_map_df[[\"category\", \"document\", \"doi\"]], on=\"document\")\n",
    "    )\n",
    "    \n",
    "    umapped_df.to_csv(\n",
    "        f\"output/embedding_output/umap/biorxiv_umap_{model_dim}.tsv\", \n",
    "        sep=\"\\t\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating document embeddings, the next step is to visualize all the documents. In order to visualize the embeddings a low dimensional representation is needed. TSNE is an another algorithm (besides UMAP) that can generate this representation, while grouping similar embeddings together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.230265Z",
     "start_time": "2020-03-20T14:53:26.495Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "random_state = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.230933Z",
     "start_time": "2020-03-20T14:53:26.498Z"
    }
   },
   "outputs": [],
   "source": [
    "for biorxiv_doc_vectors in Path().rglob(\"output/word2vec_output/biorxiv_all_articles*.tsv.xz\"):\n",
    "    model_dim = int(re.search(r\"(\\d+)\", biorxiv_doc_vectors.stem).group(1))\n",
    "    biorxiv_articles_df = pd.read_csv(str(biorxiv_doc_vectors.resolve()), sep=\"\\t\")\n",
    "    \n",
    "    reducer = TSNE(n_components=n_components, random_state=random_state)\n",
    "    \n",
    "    embedding = reducer.fit_transform(\n",
    "        biorxiv_articles_df[[f\"feat_{idx}\" for idx in range(model_dim)]].values\n",
    "    )\n",
    "    \n",
    "    tsne_df = (\n",
    "        pd.DataFrame(embedding, columns=[\"tsne1\", \"tsne2\"])\n",
    "        .assign(document=biorxiv_articles_df.document.values.tolist())\n",
    "        .merge(journal_map_df[[\"category\", \"document\", \"doi\"]], on=\"document\")\n",
    "    )\n",
    "\n",
    "    tsne_df.to_csv(\n",
    "        f\"output/embedding_output/tsne/biorxiv_tsne_{model_dim}.tsv\", \n",
    "        sep=\"\\t\", index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T17:40:16.384612Z",
     "start_time": "2020-03-23T17:40:16.382533Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "random_state = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T17:41:04.679449Z",
     "start_time": "2020-03-23T17:40:17.869906Z"
    }
   },
   "outputs": [],
   "source": [
    "for biorxiv_doc_vectors in Path().rglob(\"output/word2vec_output/biorxiv_all_articles*.tsv.xz\"):\n",
    "    model_dim = int(re.search(r\"(\\d+)\", biorxiv_doc_vectors.stem).group(1))\n",
    "    biorxiv_articles_df = pd.read_csv(str(biorxiv_doc_vectors.resolve()), sep=\"\\t\")\n",
    "    \n",
    "    reducer = PCA(\n",
    "        n_components = n_components,\n",
    "        random_state = random_state\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(\n",
    "        biorxiv_articles_df[[f\"feat_{idx}\" for idx in range(model_dim)]].values\n",
    "    )\n",
    "    \n",
    "    pca_df = (\n",
    "        pd.DataFrame(embedding, columns=[\"pca1\", \"pca2\"])\n",
    "        .assign(document=biorxiv_articles_df.document.values.tolist())\n",
    "        .merge(journal_map_df[[\"category\", \"document\", \"doi\"]], on=\"document\")\n",
    "    )\n",
    "    \n",
    "    pca_df.to_csv(\n",
    "        f\"output/embedding_output/pca/biorxiv_pca_{model_dim}.tsv\",\n",
    "        sep=\"\\t\", index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:annorxiver]",
   "language": "python",
   "name": "conda-env-annorxiver-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
